{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92b2d587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_vocabulary_aclImdb(file_name):\n",
    "    ID = 0\n",
    "    file = open(file_name, \"r\", encoding='utf8')\n",
    "    vocabulary = []\n",
    "\n",
    "    for word in file:\n",
    "        vocabulary.append(ID)\n",
    "        ID += 1\n",
    "\n",
    "    file.close()\n",
    "    return vocabulary\n",
    "    \n",
    "def read_dataset_aclImdb(file_name):\n",
    "    pos_examples = []\n",
    "    neg_examples = []\n",
    "\n",
    "    file = open(file_name, \"r\", encoding='utf8')\n",
    "    \n",
    "    content = file.read()\n",
    "    content_list = content.split(\"\\n\")\n",
    "    content_list.remove(content_list[len(content_list)-1])\n",
    "    \n",
    "    file.close()\n",
    "    \n",
    "    for review in tqdm(content_list, desc = \"Loading...\"):\n",
    "\n",
    "        IDs = []\n",
    "\n",
    "        index = int(review.index(\" \"))\n",
    "        # first number is the rank of the current review\n",
    "        rank = int(review[0:index])\n",
    "        review = review[index + 1:]\n",
    "\n",
    "        data = review.split(\" \")\n",
    "        for d in data:\n",
    "            sep = d.split(\":\")\n",
    "            IDs.append(int(sep[0]))\n",
    "\n",
    "        if rank >= 7:\n",
    "            pos_examples.append(IDs)\n",
    "        elif rank <= 4:\n",
    "            neg_examples.append(IDs)\n",
    "       \n",
    "    return pos_examples, neg_examples\n",
    "\n",
    "def convert_data(data, c):\n",
    "    x = []\n",
    "    y = []\n",
    "    for example in tqdm(data):\n",
    "        ex = []\n",
    "        for ID in vocabulary:\n",
    "            if ID in example:\n",
    "                ex.append(1)\n",
    "            else:\n",
    "                ex.append(0)\n",
    "        x.append(ex)\n",
    "        y.append(c)\n",
    "        \n",
    "        \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ca2860f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from math import log2\n",
    "\n",
    "class Information_Gain:\n",
    "\n",
    "    def __init__(self, vocabulary, positive_examples, negative_examples):\n",
    "        self.vocabulary = vocabulary\n",
    "        self.positive_examples = positive_examples\n",
    "        self.negative_examples = negative_examples\n",
    "        self.H_C = 1.0\n",
    "        self.features = []\n",
    "\n",
    "\n",
    "    def IG(self, X):\n",
    "        \"\"\"pretend that each word exists at least once in both positive\n",
    "        and negative examples, so it will not appear a math domain error in \n",
    "        log 2 function while calculating entropy\"\"\"\n",
    "        \n",
    "        X_0 = 2\n",
    "        X_1 = 2\n",
    "        C_0_X_0 = 1\n",
    "        C_1_X_0 = 1\n",
    "        C_0_X_1 = 1\n",
    "        C_1_X_1 = 1\n",
    "        \n",
    "        for example in self.positive_examples:\n",
    "            if X in example:\n",
    "                X_1 += 1\n",
    "                C_1_X_1 += 1\n",
    "            else:\n",
    "                X_0 += 1\n",
    "                C_1_X_0 += 1\n",
    "                \n",
    "        for example in self.negative_examples:\n",
    "            if X in example:\n",
    "                X_1 += 1\n",
    "                C_0_X_1 += 1\n",
    "            else:\n",
    "                X_0 += 1\n",
    "                C_0_X_0 += 1\n",
    "                \n",
    "        p_X_1 = X_1 / (len(self.positive_examples)+len(self.negative_examples))\n",
    "        p_X_0 = 1.0 - p_X_1\n",
    "        \n",
    "        p_C_0_X_1 = C_0_X_1 / X_1\n",
    "        p_C_1_X_1 = C_1_X_1 / X_1\n",
    "        p_C_0_X_0 = C_0_X_0 / X_0\n",
    "        p_C_1_X_0 = C_1_X_0 / X_0\n",
    "        \n",
    "        H_C_X_1 = -(p_C_0_X_1*log2(p_C_0_X_1) + p_C_1_X_1*log2(p_C_1_X_1))\n",
    "        H_C_X_0 = -(p_C_0_X_0*log2(p_C_0_X_0) + p_C_1_X_0*log2(p_C_1_X_0))\n",
    "        \n",
    "        S = p_X_0*H_C_X_0 + p_X_1*H_C_X_1\n",
    "        \n",
    "        return self.H_C - S\n",
    "    \n",
    "    def myFunc(self, e):\n",
    "        return e[1]\n",
    "    \n",
    "    def calculate_IG(self, n):\n",
    "        for X in tqdm(self.vocabulary[:n], desc = \"Calculating IG...\"):\n",
    "            IG_X = [X, self.IG(X)]\n",
    "            self.features.append(IG_X)\n",
    "\n",
    "        self.features.sort(key=self.myFunc, reverse=True)\n",
    "    \n",
    "    def get_m_features(self, m): \n",
    "        return [feature[0] for feature in self.features[:m]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93a5176b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_file = \"aclImdb//imdb.vocab\"\n",
    "vocabulary = read_vocabulary_aclImdb(vocabulary_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df6e4076",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading...: 100%|██████| 25000/25000 [00:04<00:00, 5494.34it/s]\n"
     ]
    }
   ],
   "source": [
    "training_file = \"aclImdb//train//labeledBow.feat\"\n",
    "\n",
    "positive_examples, negative_examples = read_dataset_aclImdb(training_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4016570",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading...: 100%|██████| 25000/25000 [00:04<00:00, 5461.12it/s]\n"
     ]
    }
   ],
   "source": [
    "testing_file = \"aclImdb//test//labeledBow.feat\"\n",
    "\n",
    "testing_positive_examples, testing_negative_examples = read_dataset_aclImdb(testing_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2177ad89",
   "metadata": {},
   "outputs": [],
   "source": [
    "I_G = Information_Gain(vocabulary, positive_examples, negative_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc5871f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating IG...: 100%|███| 5000/5000 [08:50<00:00,  9.42it/s]\n"
     ]
    }
   ],
   "source": [
    "I_G.calculate_IG(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ec55f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = I_G.get_m_features(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d31805b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████| 12500/12500 [00:47<00:00, 265.73it/s]\n",
      "100%|███████████████████| 12500/12500 [00:47<00:00, 264.14it/s]\n",
      "100%|███████████████████| 12500/12500 [00:44<00:00, 279.51it/s]\n",
      "100%|███████████████████| 12500/12500 [00:45<00:00, 276.75it/s]\n"
     ]
    }
   ],
   "source": [
    "x_pos, y_pos = convert_data(positive_examples, 1)\n",
    "x_neg, y_neg = convert_data(negative_examples, 0)\n",
    "\n",
    "    \n",
    "x_train = x_pos[:int(len(positive_examples)*80/100)] + x_neg[:int(len(positive_examples)*80/100)]\n",
    "y_train = y_pos[:int(len(positive_examples)*80/100)] + y_neg[:int(len(positive_examples)*80/100)]\n",
    "x_dev = x_pos[int(len(positive_examples)*80/100):] + x_neg[int(len(positive_examples)*80/100):]\n",
    "y_dev = y_pos[int(len(positive_examples)*80/100):] + y_neg[int(len(positive_examples)*80/100):]\n",
    "\n",
    "x_pos, y_pos = convert_data(testing_positive_examples, 1)\n",
    "x_neg, y_neg = convert_data(testing_negative_examples, 0)\n",
    "\n",
    "x_test = x_pos + x_neg\n",
    "y_test = y_pos + y_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a0573f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "random.seed = 1\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self):  \n",
    "        self.w = None\n",
    "        self.best_normalization_term = 0\n",
    "        self.threshold = 0.5\n",
    "        self.max_epochs = 5\n",
    "        self.lr_rates = [0.01, 0.001, 0.0001, 0.00001]\n",
    "        \n",
    "    \n",
    "    # is basically the probability of review to be positive\n",
    "    def __sigmoid(self, x, weights):\n",
    "        s = 0\n",
    "        for i in range(len(x)):\n",
    "            s += weights[i]*x[i]\n",
    "            \n",
    "        s += weights[len(weights)-1]\n",
    "        return 1.0 / (1.0 + np.exp(-s))\n",
    "        \n",
    "        \n",
    "    def __gradient_descent(self, weights, lr_rate, normalization_term, x, y):\n",
    "        norm_sum = 0\n",
    "        \n",
    "        for i in range(len(weights)):\n",
    "            norm_sum += weights[i]**2\n",
    "            \n",
    "        accs =[]\n",
    "        for j in range(len(x)):\n",
    "            probCPlus = self.__sigmoid(x[j], weights)\n",
    "            accs.append(y[j] - probCPlus)\n",
    "    \n",
    "        for l in range(len(weights)):\n",
    "            if l == len(weights) - 1:\n",
    "                weights[l] = (1-2*normalization_term*lr_rate)*weights[l] + lr_rate*sum(accs)\n",
    "            else:\n",
    "                anadelta = 0\n",
    "                for j in range(len(x)):\n",
    "                    anadelta += accs[j] * x[j][l]\n",
    "                weights[l] = (1-2*normalization_term*lr_rate)*weights[l] + lr_rate*anadelta\n",
    "            \n",
    "        return weights\n",
    "    \n",
    "    \n",
    "    def __calculate_w(self, x, y, n_t, lr):\n",
    "        batch_size = 64\n",
    "        total_batches = len(x) // batch_size\n",
    "        \n",
    "        batch_no = 0\n",
    "        s = 0\n",
    "        \n",
    "        for epoch in tqdm(range(self.max_epochs), desc = \"Norm_Term=\" + str(n_t) + \n",
    "                                 \" Learn_rate=\" + str(lr)):\n",
    "            \n",
    "            for batch_no in range(total_batches):\n",
    "                x_batch = x[batch_no*batch_size:(batch_no+1)*batch_size]\n",
    "                y_batch = y[batch_no*batch_size:(batch_no+1)*batch_size]\n",
    "                \n",
    "                self.w = self.__gradient_descent(self.w, lr, n_t, x_batch, y_batch)\n",
    "    \n",
    "            if (batch_no+1)*batch_size < len(x):\n",
    "                self.w = self.__gradient_descent(self.w, lr, n_t,\n",
    "                                     x[(batch_no+1)*batch_size:len(x)], \n",
    "                                     y[(batch_no+1)*batch_size:len(x)])\n",
    "            \n",
    "            norm_sum = 0\n",
    "            s = 0\n",
    "            \n",
    "            for i in range(len(self.w)):\n",
    "                norm_sum += self.w[i]**2\n",
    "            \n",
    "            for i in range(len(x)):\n",
    "                p = self.__sigmoid(x[i], self.w)\n",
    "                s += y[i]*np.log(p) + (1-y[i])*np.log(1 - p) - n_t*norm_sum\n",
    "            \n",
    "            if s == 0:\n",
    "                break\n",
    "    \n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        \n",
    "        self.w  = [random.uniform(-1,1) for i in range(len(x_train[0]) + 1)]\n",
    "        for lr in self.lr_rates:\n",
    "            self.__calculate_w(x, y, self.best_normalization_term, lr)\n",
    "    \n",
    "                \n",
    "    def fit_hyperparameters(self, x_train, y_train, x_dev, y_dev):\n",
    "        max_epochs = 10\n",
    "        \n",
    "        normalization_terms = [0.05, 0.1, 0.15, 0.2]\n",
    "        \n",
    "        best_dev_acc = -1\n",
    "\n",
    "        for n_t in normalization_terms:\n",
    "            self.w  = [random.uniform(-1,1) for i in range(len(x_train[0]) + 1)]\n",
    "            for lr in self.lr_rates:\n",
    "                self.__calculate_w(x_train, y_train, n_t, lr)\n",
    "                    \n",
    "            current_dev_acc = self.accurate(x_dev, y_dev, False)\n",
    "                    \n",
    "            if current_dev_acc > best_dev_acc:\n",
    "                best_dev_acc = current_dev_acc\n",
    "                self.best_normalization_term = n_t\n",
    "                \n",
    "                \n",
    "            \n",
    "    def accurate(self, x, y, All = False):\n",
    "        total = 0\n",
    "        TP = 0\n",
    "        FP = 0\n",
    "        FN = 0\n",
    "        for i in range(len(x)):\n",
    "            prob = self.__sigmoid(x[i], self.w)\n",
    "            clf = -1\n",
    "            if prob >= self.threshold:\n",
    "                clf = 1\n",
    "            elif prob <= 1 - self.threshold:\n",
    "                clf = 0\n",
    "            if int(y[i]) == clf:\n",
    "                total+=1\n",
    "            \n",
    "            if clf == 1:\n",
    "                if int(y[i]) == 1:\n",
    "                    TP += 1\n",
    "                else:\n",
    "                    FP += 1\n",
    "            else:\n",
    "                if int(y[i]) == 1:\n",
    "                    FN += 1        \n",
    "        \n",
    "        if All:\n",
    "            Precision = TP / (TP + FP)\n",
    "            Recall = TP / (TP + FN)\n",
    "            F1 = 2*Precision*Recall / (Precision+Recall)\n",
    "            return total/len(x), Precision, Recall, F1\n",
    "        else: \n",
    "            return total/len(x)\n",
    "        \n",
    "        \n",
    "    def predict(self, x):\n",
    "        predictions = []\n",
    "        \n",
    "        for i in range(len(x)):\n",
    "            prob = self.__sigmoid(x[i], self.w)\n",
    "            clf = -1\n",
    "            if prob >= self.threshold:\n",
    "                clf = 1\n",
    "            elif prob <= 1 - self.threshold:\n",
    "                clf = 0\n",
    "            \n",
    "            predictions.append(clf)\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ee66b4a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Norm_Term=0.05 Learn_rate=0.01: 100%|█| 5/5 [01:58<00:00, 23.72\n",
      "Norm_Term=0.05 Learn_rate=0.001: 100%|█| 5/5 [02:01<00:00, 24.2\n",
      "Norm_Term=0.05 Learn_rate=0.0001: 100%|█| 5/5 [02:09<00:00, 25.\n",
      "Norm_Term=0.05 Learn_rate=1e-05: 100%|█| 5/5 [03:28<00:00, 41.7\n",
      "Norm_Term=0.1 Learn_rate=0.01: 100%|█| 5/5 [03:49<00:00, 45.81s\n",
      "Norm_Term=0.1 Learn_rate=0.001: 100%|█| 5/5 [03:47<00:00, 45.58\n",
      "Norm_Term=0.1 Learn_rate=0.0001: 100%|█| 5/5 [02:58<00:00, 35.7\n",
      "Norm_Term=0.1 Learn_rate=1e-05: 100%|█| 5/5 [02:14<00:00, 26.91\n",
      "Norm_Term=0.15 Learn_rate=0.01: 100%|█| 5/5 [02:43<00:00, 32.79\n",
      "Norm_Term=0.15 Learn_rate=0.001: 100%|█| 5/5 [03:39<00:00, 43.9\n",
      "Norm_Term=0.15 Learn_rate=0.0001: 100%|█| 5/5 [03:37<00:00, 43.\n",
      "Norm_Term=0.15 Learn_rate=1e-05: 100%|█| 5/5 [03:34<00:00, 42.9\n",
      "Norm_Term=0.2 Learn_rate=0.01: 100%|█| 5/5 [03:29<00:00, 41.98s\n",
      "Norm_Term=0.2 Learn_rate=0.001: 100%|█| 5/5 [03:45<00:00, 45.17\n",
      "Norm_Term=0.2 Learn_rate=0.0001: 100%|█| 5/5 [03:47<00:00, 45.5\n",
      "Norm_Term=0.2 Learn_rate=1e-05: 100%|█| 5/5 [03:09<00:00, 37.96\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit_hyperparameters(x_train, y_train, x_dev, y_dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2dd73e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n"
     ]
    }
   ],
   "source": [
    "print(lr.best_normalization_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f69e15af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Norm_Term=0.1 Learn_rate=0.01: 100%|█| 5/5 [00:22<00:00,  4.53s\n",
      "Norm_Term=0.1 Learn_rate=0.001: 100%|█| 5/5 [00:21<00:00,  4.38\n",
      "Norm_Term=0.1 Learn_rate=0.0001: 100%|█| 5/5 [00:22<00:00,  4.5\n",
      "Norm_Term=0.1 Learn_rate=1e-05: 100%|█| 5/5 [00:22<00:00,  4.59\n",
      "Norm_Term=0.1 Learn_rate=0.01: 100%|█| 5/5 [00:45<00:00,  9.14s\n",
      "Norm_Term=0.1 Learn_rate=0.001: 100%|█| 5/5 [00:45<00:00,  9.12\n",
      "Norm_Term=0.1 Learn_rate=0.0001: 100%|█| 5/5 [00:45<00:00,  9.1\n",
      "Norm_Term=0.1 Learn_rate=1e-05: 100%|█| 5/5 [00:43<00:00,  8.63\n",
      "Norm_Term=0.1 Learn_rate=0.01: 100%|█| 5/5 [01:08<00:00, 13.65s\n",
      "Norm_Term=0.1 Learn_rate=0.001: 100%|█| 5/5 [01:08<00:00, 13.72\n",
      "Norm_Term=0.1 Learn_rate=0.0001: 100%|█| 5/5 [01:08<00:00, 13.7\n",
      "Norm_Term=0.1 Learn_rate=1e-05: 100%|█| 5/5 [01:08<00:00, 13.70\n",
      "Norm_Term=0.1 Learn_rate=0.01: 100%|█| 5/5 [01:30<00:00, 18.19s\n",
      "Norm_Term=0.1 Learn_rate=0.001: 100%|█| 5/5 [01:30<00:00, 18.18\n",
      "Norm_Term=0.1 Learn_rate=0.0001: 100%|█| 5/5 [01:31<00:00, 18.2\n",
      "Norm_Term=0.1 Learn_rate=1e-05: 100%|█| 5/5 [01:30<00:00, 18.15\n",
      "Norm_Term=0.1 Learn_rate=0.01: 100%|█| 5/5 [01:53<00:00, 22.74s\n",
      "Norm_Term=0.1 Learn_rate=0.001: 100%|█| 5/5 [01:53<00:00, 22.78\n",
      "Norm_Term=0.1 Learn_rate=0.0001: 100%|█| 5/5 [01:54<00:00, 22.9\n",
      "Norm_Term=0.1 Learn_rate=1e-05: 100%|█| 5/5 [01:36<00:00, 19.23\n",
      "Norm_Term=0.1 Learn_rate=0.01: 100%|█| 5/5 [02:25<00:00, 29.11s\n",
      "Norm_Term=0.1 Learn_rate=0.001: 100%|█| 5/5 [03:36<00:00, 43.31\n",
      "Norm_Term=0.1 Learn_rate=0.0001: 100%|█| 5/5 [03:32<00:00, 42.4\n",
      "Norm_Term=0.1 Learn_rate=1e-05: 100%|█| 5/5 [02:23<00:00, 28.76\n",
      "Norm_Term=0.1 Learn_rate=0.01: 100%|█| 5/5 [02:38<00:00, 31.79s\n",
      "Norm_Term=0.1 Learn_rate=0.001: 100%|█| 5/5 [02:39<00:00, 31.84\n",
      "Norm_Term=0.1 Learn_rate=0.0001: 100%|█| 5/5 [02:39<00:00, 31.8\n",
      "Norm_Term=0.1 Learn_rate=1e-05: 100%|█| 5/5 [02:39<00:00, 31.82\n",
      "Norm_Term=0.1 Learn_rate=0.01: 100%|█| 5/5 [02:27<00:00, 29.57s\n",
      "Norm_Term=0.1 Learn_rate=0.001: 100%|█| 5/5 [01:55<00:00, 23.19\n",
      "Norm_Term=0.1 Learn_rate=0.0001: 100%|█| 5/5 [01:39<00:00, 19.9\n",
      "Norm_Term=0.1 Learn_rate=1e-05: 100%|█| 5/5 [02:04<00:00, 24.91\n",
      "Norm_Term=0.1 Learn_rate=0.01: 100%|█| 5/5 [03:24<00:00, 40.89s\n",
      "Norm_Term=0.1 Learn_rate=0.001: 100%|█| 5/5 [03:06<00:00, 37.38\n",
      "Norm_Term=0.1 Learn_rate=0.0001: 100%|█| 5/5 [02:03<00:00, 24.6\n",
      "Norm_Term=0.1 Learn_rate=1e-05: 100%|█| 5/5 [02:58<00:00, 35.68\n",
      "Norm_Term=0.1 Learn_rate=0.01: 100%|█| 5/5 [03:45<00:00, 45.16s\n",
      "Norm_Term=0.1 Learn_rate=0.001: 100%|█| 5/5 [03:46<00:00, 45.22\n",
      "Norm_Term=0.1 Learn_rate=0.0001: 100%|█| 5/5 [02:56<00:00, 35.2\n",
      "Norm_Term=0.1 Learn_rate=1e-05: 100%|█| 5/5 [02:14<00:00, 26.80\n"
     ]
    }
   ],
   "source": [
    "x_train_length = np.arange(2000, len(x_train)+1, 2000)\n",
    "\n",
    "train_accuracy = []\n",
    "dev_accuracy = []\n",
    "test_accuracy = []\n",
    "precision = []\n",
    "recall = []\n",
    "F1 = []\n",
    "\n",
    "for length in x_train_length:\n",
    "    lr.fit(x_train[:length], y_train[:length])\n",
    "    \n",
    "    train_accuracy.append(lr.accurate(x_train[:length], y_train[:length]))\n",
    "    dev_accuracy.append(lr.accurate(x_dev, y_dev))\n",
    "    \n",
    "    curr_test_acc, curr_precision, curr_recall, curr_F1 = lr.accurate(x_test, y_test, True)\n",
    "    \n",
    "    test_accuracy.append(curr_test_acc)\n",
    "    precision.append(curr_precision)\n",
    "    recall.append(curr_recall)\n",
    "    F1.append(curr_F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "09ca917f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "ax = plt.subplot()\n",
    "plt.plot(x_train_length, train_accuracy, 'g-o', label=\"TRAIN_ACCURACY\")\n",
    "plt.plot(x_train_length, test_accuracy, 'r-o', label=\"TEST_ACCURACY\")\n",
    "\n",
    "plt.ylabel(\"ACCURACY\\n\")\n",
    "plt.xlabel(\"\\n NUM OF TRAINING EXAMPLES\")\n",
    "\n",
    "plt.ylim(0,1)\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig(\"curves//train_test_accuracy.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a46c12e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "ax = plt.subplot()\n",
    "\n",
    "plt.plot(x_train_length, precision, 'g-o', label = \"PRECISION\")\n",
    "plt.plot(x_train_length, recall, 'r-o', label = \"RECALL\")\n",
    "plt.plot(x_train_length, F1, 'b-o', label = \"F1\")\n",
    "\n",
    "plt.xlabel(\"\\n NUM OF TRAINING EXAMPLES\")\n",
    "\n",
    "plt.ylim(0,1)\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig(\"curves//Recall_Precision_F1.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7cfa389c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------+---------------+\n",
      "| Training examples |   Train Accuracy   | Dev Accuracy | Test Accuracy |\n",
      "+-------------------+--------------------+--------------+---------------+\n",
      "|        2000       |        1.0         |     0.5      |      0.5      |\n",
      "|        4000       |        1.0         |     0.5      |    0.50004    |\n",
      "|        6000       |        1.0         |     0.5      |      0.5      |\n",
      "|        8000       |        1.0         |     0.5      |      0.5      |\n",
      "|       10000       |        1.0         |     0.5      |      0.5      |\n",
      "|       12000       |        0.89        |    0.728     |    0.72916    |\n",
      "|       14000       | 0.8582142857142857 |    0.7984    |     0.8062    |\n",
      "|       16000       |      0.842875      |    0.827     |    0.82652    |\n",
      "|       18000       | 0.8353333333333334 |    0.8318    |     0.8276    |\n",
      "|       20000       |       0.8311       |    0.8242    |    0.82088    |\n",
      "+-------------------+--------------------+--------------+---------------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "accuracy_table = PrettyTable([\"Training examples\", \"Train Accuracy\", \"Dev Accuracy\", \"Test Accuracy\"])\n",
    "\n",
    "for i in range(len(x_train_length)):\n",
    "    accuracy_table.add_row([x_train_length[i], train_accuracy[i], dev_accuracy[i], test_accuracy[i]])\n",
    "\n",
    "print(accuracy_table)\n",
    "\n",
    "file = open(\"tables//accuracy_table.txt\", \"w\")\n",
    "file.write(str(accuracy_table))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e52ccf8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+---------+--------------------+\n",
      "| Training examples |     Precision      |  Recall |         F1         |\n",
      "+-------------------+--------------------+---------+--------------------+\n",
      "|        2000       |        0.5         | 0.99984 | 0.6666311073181139 |\n",
      "|        4000       | 0.500020000800032  |   1.0   | 0.6666844449185311 |\n",
      "|        6000       |        0.5         |   1.0   | 0.6666666666666666 |\n",
      "|        8000       |        0.5         |   1.0   | 0.6666666666666666 |\n",
      "|       10000       |        0.5         |   1.0   | 0.6666666666666666 |\n",
      "|       12000       | 0.6545954989475956 | 0.97032 | 0.7817847819781495 |\n",
      "|       14000       | 0.7498531235720347 | 0.91896 | 0.8258384557316942 |\n",
      "|       16000       | 0.8056158742044178 | 0.86072 | 0.832256816863276  |\n",
      "|       18000       | 0.8430222817892444 | 0.80512 | 0.8236353220394467 |\n",
      "|       20000       | 0.8739511467462241 | 0.74992 | 0.8071988288986481 |\n",
      "+-------------------+--------------------+---------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "precision_recall_F1_table = PrettyTable([\"Training examples\", \"Precision\", \"Recall\", \"F1\"])\n",
    "\n",
    "for i in range(len(x_train_length)):\n",
    "    precision_recall_F1_table.add_row([x_train_length[i], precision[i], recall[i], F1[i]])\n",
    "\n",
    "print(precision_recall_F1_table)\n",
    "\n",
    "file = open(\"tables//precision_recall_F1_table.txt\", \"w\")\n",
    "file.write(str(precision_recall_F1_table))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eef83966",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/user/0/ru.iiec.pydroid3/files/aarch64-linux-android/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit our training data to sklearn.LogisticRegression model, in order to compare the results with ours.\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "log = LogisticRegression()\n",
    "log.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cea7bdec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.87      0.87     12500\n",
      "           1       0.87      0.88      0.87     12500\n",
      "\n",
      "    accuracy                           0.87     25000\n",
      "   macro avg       0.87      0.87      0.87     25000\n",
      "weighted avg       0.87      0.87      0.87     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sklearn.LogisticRegression report \n",
    "print(classification_report(y_test, log.predict(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "325709d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.89      0.83     12500\n",
      "           1       0.87      0.75      0.81     12500\n",
      "\n",
      "    accuracy                           0.82     25000\n",
      "   macro avg       0.83      0.82      0.82     25000\n",
      "weighted avg       0.83      0.82      0.82     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# our report \n",
    "print(classification_report(y_test, lr.predict(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917c917c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
